{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30580,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installations","metadata":{}},{"cell_type":"code","source":"!pip install -q packaging ninja\n!pip install -q flash-attn --no-build-isolation\n!pip install -q -U bitsandbytes transformers accelerate\n\n!pip install -q gdown","metadata":{"execution":{"iopub.status.busy":"2023-11-14T05:54:27.908558Z","iopub.execute_input":"2023-11-14T05:54:27.908838Z","iopub.status.idle":"2023-11-14T05:55:34.293940Z","shell.execute_reply.started":"2023-11-14T05:54:27.908813Z","shell.execute_reply":"2023-11-14T05:55:34.292715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data processing","metadata":{}},{"cell_type":"code","source":"import nltk\nimport gdown\nimport pandas as pd\n\nlinks = {\"filtered_keyword_sentences.csv\": \"https://drive.google.com/file/d/1d4pLs1K69HcvOA9WAuc5g0l1qXtbBBIa/view?usp=sharing\",\n         \"filtered_keyword_sentences_sample.csv\": \"https://drive.google.com/file/d/1mLBg9MAcd90y4lU2xQqmXXVXRX1Bdrm-/view?usp=sharing\"}\nfor name, link in links.items():\n    url = 'https://drive.google.com/uc?id=' + link.split('/')[5]\n    gdown.download(url, name)\n\n# Pull extracted sentences from keywords\nsentences = []\n\ndf = pd.read_csv(\"/kaggle/working/filtered_keyword_sentences.csv\")\nsentences += df[\"Extracted Sentence\"].to_list()\nprint(len(sentences))\n\ndf = pd.read_csv(\"/kaggle/working/filtered_keyword_sentences_sample.csv\")\nsentences += df[\"Extracted Sentence\"].to_list()\nprint(len(sentences))\n\nsentences = set(sentences)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T05:55:54.638161Z","iopub.execute_input":"2023-11-14T05:55:54.638524Z","iopub.status.idle":"2023-11-14T05:55:57.198878Z","shell.execute_reply.started":"2023-11-14T05:55:54.638495Z","shell.execute_reply":"2023-11-14T05:55:57.197996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# APPEALING TO EMOTION\nemotion = '''\nAppealing to emotion, a fallacy category, is described as \"manipulation of the recipient’s emotions in order to win an argument\". Its loose logical form is: \"[CLAIM] is made without evidence. In place of evidence, emotion is used to convince the interlocutor that [CLAIM] is true\". Your task is to make a political-related sentence to become fallacious, using the provided description and the loose logical form. Based on examples #1 to #7 below, please complete a fallacy for sentence #8:\n\nSource #1: Illegal immigration hurts the economy\nFallacy #1: \"There's a direct connection between illegal immigration and the downfall of the economy. Just look at how it lead to increase in crime, the world has never been this dangerous before!\"\nSource #2: Global integration exposes deep fault lines in the existing international order.\nFallacy #2: \"Let's not forget the history of global integration. It's always been about exploitation, and the exploitation of our resources will always come at a cost.\"\nSource #3: We expect the United Nations to play an effective and vital role in resolving conflicts.\nFallacy #3: \"The United Nations is the only hope for peace in the world. We can't afford to let it fail us. Don't you care about the safety and security of your children and grandchildren?\"\nSource #4: Citizens are demanding respect for the dignity of all people.\nFallacy #4: \"Respect for the dignity of all people is a threat to the safety and security of our nation. We can't afford to entertain such demands, especially when they come from those who don't share our values.\"\nSource #5: Global warming is not real\nFallacy #5: \"Nature has its own way of balancing things out, and global warming is nothing more than just part of a natural cycle.\"\nSource #6: Globalization, while raising global living standards, has weakened workers’ positions and their ability to earn a decent wage.\nFallacy #6: \"Globalization may have brought some benefits, but it has also led to the exploitation of workers and their inability to earn a decent wage. Don't you care about the future of our economy and the well-being of our people?\"\nSource #7: Advancements in science and technology, along with political changes, have significantly improved the quality of life for people globally and increased access to knowledge and freedom.\nFallacy #7: \"But at what cost? Every new breakthrough in technology brings us closer to a world where our privacy is constantly violated.\"\nSource #8:'''\n\n# FALSE DILEMMA\ndilemma = '''\nFalse dilemma, a fallacy category, is described as \"A claim presenting only two options or sides when there are many options or sides\". Your task is to make a political-related sentence to become fallacious, using the provided description and the loose logical form. Based on examples #1 to #4 below, please complete a fallacy for sentence #5:\n\nSource #1: Global warming is not real.\nFallacy #1: \"Global warming can't be a real thing. Just look at how nature has its own way of balancing things out throughout the history of the universe.\"\nSource #2: America's potential as a net energy exporter in the near future should not be jeopardized.\nFallacy #2: \"America is poised to become a net energy exporter over the next decade. We should not abandon that progress at the cost of weakening our energy renaissance and crippling economic growth.\"\nSource #3: Taking action to address climate change is important.\nFallacy #3: \"I don’t want to give up my car, so I don’t think I can support fighting climate change.\"\nSource #4: Illegal immigration hurts the economy.\nFallacy #4: \"There are only two options when it comes to illegal immigration - either we allow it and harm the economy or we deport everyone and disrupt families.\"\nSource #5:'''\n\n# FALLACY OF RELEVANCE\nrelevancy = '''\nFallacy of Relevance, also known as red herring, occurs when the speaker attempts to divert attention from the primary argument by offering a point that does not suffice as counterpoint/supporting evidence (even if it is true). Its loose logical form is: \"It is claimed that [MSK1] implies [MSK2], whereas [MSK1] is unrelated to [MSK2]\". Your task is to make a source sentence to become fallacious, using the provided description and the loose logical form. Based on examples #1 to #7, please complete a fallacy for sentence #8:\n\nSource #1: Citizens are demanding respect for the dignity of all people.\nFallacy #1: \"We need to focus more on national security and economic growth. The respect for the dignity of all people is a luxury that is only affordable after a nation is stable and prosperous.\"\nSource #2: We cannot trust the government. They have been making so many mistakes that have jeopardized our rights and freedoms.\nFallacy #2: \"There's no reason to doubt the government’s actions. People are not capable of defending themselves either, they have no right to criticize.\"\nSource #3: Globalization has weakened workers’ positions and their ability to earn a decent wage.\nFallacy #3: \"Globalization has increased access to a wider range of products and services, which is beneficial for consumers. So it should not be a problem even though it negatively affects people's income.\"\nSource #4: Illegal immigration hurts the economy.\nFallacy #4: \"People that illegally migrate to our country use more public services than they pay in taxes, thus they have bad impact on the economy.\"\nSource #5: There's a big concern on the financial burden from relocating the office to California.\nFallacy #5: \"The weather in California is so much warmer, we must move the office there.\"\nSource #6: Several employees were dissapointed because they weren't promoted as promised by the compaany.\nFallacy #6: \"It's okay for the company to not raise salaries since they still provide great benefits for the employees.\"\nSource #7: During the politician's tenure, the party was heavily corrupted.\nFallacy #7: \"There was a little issue of corruption last year, but look at how much more corrupt they are in the other party!\"\"\nSource #8:'''\n\n# INTENTIONAL FALLACY\nintention ='''\nIntentional fallcy is a fallacy category for when an argument has some element that shows intent of a speaker to win an argument without actual supporting evidence. Its loose logical form is: \"[MSK1] knows [MSK2] is incorrect. [MSK1] still claim that [MSK2] is correct using an incorrect argument.\" Your task is to make a source sentence to become fallacious, using the provided description and the loose logical form. Based on examples #1 to #2, please complete a fallacy for sentence #3:\n\nSource #1: The government has been withholding the information about extraterrestrials existence from us for its own interests.\nFallacy #1: “No one has ever been able to prove that extraterrestrials exist, so they must not be real.\"\nSource #2: Using positive discipline builds better connections with kids, questioning the idea that hitting them is both effective and ethical.\nFallacy #2: \"It’s common sense that if you smack your children, they will stop the bad behavior. So don’t tell me not to hit my kids.\"\nSource #3:'''","metadata":{"execution":{"iopub.status.busy":"2023-11-14T06:06:37.310166Z","iopub.execute_input":"2023-11-14T06:06:37.310550Z","iopub.status.idle":"2023-11-14T06:06:37.322214Z","shell.execute_reply.started":"2023-11-14T06:06:37.310519Z","shell.execute_reply":"2023-11-14T06:06:37.321148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_prompts(sentence_list, fallacy):\n    \"\"\"\n    Parameters\n    ----------\n    sentences : str\n        List of compact sentences to build prompt for\n    fallacy:\n        Fallacy-specific prompt, with form:\n        \n        <fallacy description>, <loose logical form>, <task description>\n        Source #1: <sample source 1>\n        Fallacy #1: <sample corresponding fallacy 1>\n        Source #2: <sample source 2>\n        Fallacy #2: <sample corresponding fallacy 2>\n        ...\n        Source #k:\n    \n    Return\n    ------\n    prompts:\n        Prompt dataset, each entry sentence[i] has the form:\n        \n        <fallacy description>, <loose logical form>, <task description>\n        Source #1: <sample source 1>\n        Fallacy #1: <sample corresponding fallacy 1>\n        Source #2: <sample source 2>\n        Fallacy #2: <sample corresponding fallacy 2>\n        ...\n        Source #k: <sentence[i]>\n        Fallacy #k:\n    \"\"\"\n\n    prompts = []\n    for i in sentence_list:\n        i = i.replace(\"\\n\", \" \")\n        prompt = \"[INST] \" + fallacy + \" \" + i + f\"\\nFallacy #{int(fallacy[-2])}: \" + \"\\n[/INST]\\n\"\n        prompts.append({\"source\": i, \"prompt\": prompt})\n    \n    return prompts","metadata":{"execution":{"iopub.status.busy":"2023-11-14T07:44:39.972221Z","iopub.execute_input":"2023-11-14T07:44:39.973207Z","iopub.status.idle":"2023-11-14T07:44:39.979828Z","shell.execute_reply.started":"2023-11-14T07:44:39.973174Z","shell.execute_reply":"2023-11-14T07:44:39.978681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fallacy_prompts = [emotion, dilemma, relevancy, intention]\n\n# prompt_datasets: num_fallacy_type x num_sentences\nprompt_datasets = [make_prompts(sentences, f) for f in fallacy_prompts]","metadata":{"execution":{"iopub.status.busy":"2023-11-14T07:44:41.358531Z","iopub.execute_input":"2023-11-14T07:44:41.359518Z","iopub.status.idle":"2023-11-14T07:44:41.366240Z","shell.execute_reply.started":"2023-11-14T07:44:41.359486Z","shell.execute_reply":"2023-11-14T07:44:41.365314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"E.g: `prompt_datasets[1][0][\"prompt\"]` looks like below:\n```python\n\"\"\"\n[INST] \nFalse dilemma, a fallacy category, is described as \"A claim presenting only two options or sides when there are many options or sides\". Your task is to make a political-related sentence to become fallacious, using the provided description and the loose logical form. Based on examples #1 to #4 below, please complete a fallacy for sentence #5:\n\nSource #1: Global warming is not real.\nFallacy #1: \"Global warming can't be a real thing. Just look at how nature has its own way of balancing things out throughout the history of the universe.\"\nSource #2: America's potential as a net energy exporter in the near future should not be jeopardized.\nFallacy #2: \"America is poised to become a net energy exporter over the next decade. We should not abandon that progress at the cost of weakening our energy renaissance and crippling economic growth.\"\nSource #3: Taking action to address climate change is important.\nFallacy #3: \"I don’t want to give up my car, so I don’t think I can support fighting climate change.\"\nSource #4: Illegal immigration hurts the economy.\nFallacy #4: \"There are only two options when it comes to illegal immigration - either we allow it and harm the economy or we deport everyone and disrupt families.\"\nSource #5: And the world is too small for us to simply be able to buil The world is too small for us to simply be able to build a wall and prevent it from affecting our own societies.\nFallacy #5: \n[/INST]\n\"\"\"\n```","metadata":{}},{"cell_type":"markdown","source":"# Model preparation","metadata":{}},{"cell_type":"markdown","source":"## Load models and tokenizer","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    AutoTokenizer,\n    GenerationConfig\n)\n\n#Quantization configuration\ncompute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True,\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T06:26:49.651508Z","iopub.execute_input":"2023-11-14T06:26:49.651881Z","iopub.status.idle":"2023-11-14T06:26:53.268195Z","shell.execute_reply.started":"2023-11-14T06:26:49.651852Z","shell.execute_reply":"2023-11-14T06:26:53.267445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\ntokenizer.pad_token = tokenizer.unk_token\ntokenizer.pad_token_id =  tokenizer.unk_token_id\ntokenizer.padding_side = 'left'\n\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\",\n                                             quantization_config=bnb_config,\n                                             load_in_4bit=True,\n                                             device_map=\"auto\")\n#Configure the pad token in the model\nmodel.config.pad_token_id = tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2023-11-14T06:26:53.269576Z","iopub.execute_input":"2023-11-14T06:26:53.270046Z","iopub.status.idle":"2023-11-14T06:28:54.634091Z","shell.execute_reply.started":"2023-11-14T06:26:53.270017Z","shell.execute_reply":"2023-11-14T06:28:54.633048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_config = GenerationConfig(\n    pad_token_id=tokenizer.pad_token_id,\n    temperature=0.7,\n    top_p=1.0,\n    top_k=50,\n    do_sample=True,\n    num_beams=1\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T06:29:21.466998Z","iopub.execute_input":"2023-11-14T06:29:21.467392Z","iopub.status.idle":"2023-11-14T06:29:21.472343Z","shell.execute_reply.started":"2023-11-14T06:29:21.467361Z","shell.execute_reply":"2023-11-14T06:29:21.471350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"import time\n\nBATCH_SIZE = 5\n\ndef inference(dataset):\n    \"\"\"\n    Batch inference on dataset of fallacy prompts\n\n    Parameters\n    ----------\n    dataset:\n        list of prompts of a specific fallacy type. Each entry is a\n        dictionary: {\"source\": <the source sentence>, \"prompt\": <the constructed prompt for the source sentence>}\n    \n    Returns\n    -------\n    augmented_results:\n        list of augmented premises. Each entry is a dictionary:\n        {\"source\": <the source sentence>, \"premise\": <augmented premises for the source sentence>}\n    \"\"\"\n    augmented_results = []\n    for i in range(0, len(dataset), BATCH_SIZE):\n        start = time.time()\n        end_id = min(i+BATCH_SIZE, len(dataset)-1)\n        \n        prompts = [d[\"prompt\"] for d in dataset[i:end_id]]\n        sources = [d[\"source\"] for d in dataset[i:end_id]]\n        model_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n        with torch.no_grad():\n            generated_ids = model.generate(**model_inputs,\n                                           max_new_tokens=200,\n                                           generation_config=gen_config)\n        generated_texts = tokenizer.batch_decode(generated_ids)\n        \n        augmented_results += [{\"source\": source, \"premise\": premise} for source, premise in zip(sources, generated_texts)]\n        print(f\"dataset[{i}:{end_id}] after {time.time() - start} seconds\")\n    print(\"Augmentation done!\\n\\n\\n\")\n    return augmented_results\n\n# Test\n# augmented_premises = [inference(dataset) for dataset in [prompt_datasets[0][:3], prompt_datasets[1][:3], prompt_datasets[2][:3], prompt_datasets[3][:3]]]\n# Augment each fallacy-specific prompt dataset\naugmented_premises = [inference(dataset) for dataset in prompt_datasets] # num_fallacy_type x num_sentences","metadata":{"execution":{"iopub.status.busy":"2023-11-14T07:50:01.509727Z","iopub.execute_input":"2023-11-14T07:50:01.510639Z","iopub.status.idle":"2023-11-14T07:51:06.221182Z","shell.execute_reply.started":"2023-11-14T07:50:01.510603Z","shell.execute_reply":"2023-11-14T07:51:06.220092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open(\"/kaggle/working/checkpoint.json\", \"w\") as f:\n    json.dump(augmented_premises, f)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T09:06:51.615342Z","iopub.execute_input":"2023-11-14T09:06:51.616345Z","iopub.status.idle":"2023-11-14T09:06:51.621370Z","shell.execute_reply.started":"2023-11-14T09:06:51.616306Z","shell.execute_reply":"2023-11-14T09:06:51.620393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\nFALLACY_HASH = [\"emotion\", \"dilemma\", \"relevancy\", \"intention\"]\n\ndef extract_premises(augmented):\n    \"\"\"\n    Parameters\n    ----------\n    augmented: list(dict())\n        list of {\"source\": <src sentence>, \"premise\": <augmented premise>}\n    \n    Returns\n    -------\n    premises:\n        augmented premises that's enclosed by \" \" in generated outputs\n    premise_sources:\n        source sentences of the premises\n    errors:\n        augmented premises that don't follow format due to uncontrollable generated\n    error_sources:\n        source sentences of the error premises\n    \"\"\"\n    premises = []\n    is_error = []\n    sources = []\n    for aug in augmented:\n        try:\n            s = aug[\"premise\"].replace(\"\\n\", \" \")\n            g = re.findall(r'\\[/INST].*', s)\n            premises.append(g[0].split('\"')[-2])\n            is_error.append(\"-\")\n        except:\n            g = g[0].replace(\"[/INST]\", \"\")\n            g = g.replace(\"<unk>\", \"\")\n            g = g.replace(\"</s>\", \"\")\n            premises.append(g)\n            is_error.append(\"+\")\n        sources.append(aug[\"source\"])\n\n    assert len(premises) == len(sources) == len(is_error)\n    return premises, sources, is_error","metadata":{"execution":{"iopub.status.busy":"2023-11-14T09:01:07.910927Z","iopub.execute_input":"2023-11-14T09:01:07.911665Z","iopub.status.idle":"2023-11-14T09:01:07.919881Z","shell.execute_reply.started":"2023-11-14T09:01:07.911632Z","shell.execute_reply":"2023-11-14T09:01:07.918831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dataframe\npremises = []\nsources = []\nis_errors = []\n\nfor augmented in augmented_premises: # each loop corresponds to a fallacy type\n    premise, source, is_error = extract_premises(augmented)\n    premises.append(premise)\n    sources = source\n    is_errors.append(is_error)\n\ndata = {\"source\": source}\nfor t in range(len(premises)):\n    data[f\"p_{FALLACY_HASH[t]}\"] = premises[t]\n    data[f\"p_{FALLACY_HASH[t]}_is_error\"] = is_errors[t]\n    \ndf = pd.DataFrame(data)\n\ndf.to_csv(\"/kaggle/working/premises.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-11-14T09:01:22.415679Z","iopub.execute_input":"2023-11-14T09:01:22.416426Z","iopub.status.idle":"2023-11-14T09:01:22.424786Z","shell.execute_reply.started":"2023-11-14T09:01:22.416387Z","shell.execute_reply":"2023-11-14T09:01:22.423916Z"},"trusted":true},"execution_count":null,"outputs":[]}]}